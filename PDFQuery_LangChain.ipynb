{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0LoofyYitx0",
        "outputId": "8ac9ce3f-4ec5-460e-9e8a-ddcddd5de41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip setuptools\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PvjJPfEViCr5"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEkTrJTzkHAj",
        "outputId": "2dbf3b2d-3d09-494c-dee3-8251acaff8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain-community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.6)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.17\n",
            "    Uninstalling langchain-0.3.17:\n",
            "      Successfully uninstalled langchain-0.3.17\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain-community-0.3.17 langchain-core-0.3.35 langchain-text-splitters-0.3.6 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgK0bAL8iRZ4"
      },
      "source": [
        "Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jiKIyPuniYSG"
      },
      "outputs": [],
      "source": [
        "# LangChain components to use\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4u6SKjGkW9y",
        "outputId": "2e74f3a1-6d27-4903-ab82-6280799508d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hxzGp5bmkkyj"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb2NSVG1kttP"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpxV6cfWkxF-"
      },
      "source": [
        "Provide your secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bVopEfAkqog"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:KLQmJDgJfEQhTHesdRftkYGJAfm:69b674fee5aa4fe9c423205caa063e3a275e8285b969938bacbdff766a305f09\"\n",
        "ASTRA_DB_ID = \"bh34dfe89-ad04-4127-bb22-c6885416ea03\"\n",
        "\n",
        "OPENAI_API_KEY = \"sk-proj-YKMitMOmtVfghjkiu3XC4jYCxs7ySHmt6qfEru3azDP1azYhesdfghjn8cUIDnIvus_-T1rfSaIiJvTEXL6eoT3BlbkFJoLiuzxG2Tfg3WoMk41hIRTIQ21jIiTRl4OtjKfKmxBhdQzQFcciiZ-B0hGz-2V40DAwq-ICakA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U16vEZsgl4fx"
      },
      "outputs": [],
      "source": [
        "pdfreader = PdfReader('/content/Bsdsf21a028_Lab1_Report.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "flziggirmfKg"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Concatenate\n",
        "\n",
        "# Read text from PDF\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "dUwKIpNim8YA",
        "outputId": "0730164a-ba28-4b7b-dbf3-bc391fd9ad1b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Facebook\\nData\\nPerformance\\nAnalysis\\nReport\\nIntroduction\\nThis\\nreport\\naims\\nto\\nanalyze\\nthe\\nperformance\\nof\\nSQL\\nqueries\\non\\nvarious\\ndatasets\\nsimulating\\nFacebook-related\\ndata\\nsuch\\nas\\nuser\\nprofiles,\\nposts,\\ncomments,\\nand\\ninteractions.\\nThe\\ngoal\\nis\\nto\\nevaluate\\nthe\\nperformance\\nof\\neach\\nquery\\nacross\\ndifferent\\ndata\\nsizes,\\nusing\\nexecution\\ntimes\\nas\\na\\nmetric\\nfor\\ncomparison.\\nTable\\nDescriptions\\nand\\nData\\nPopulation\\n:\\nObjective\\n:\\nCreate\\nand\\npopulate\\ntables\\nto\\nstore\\nFacebook-related\\ndata,\\nreflecting\\nrealistic\\nscenarios.\\nTables\\nCreated\\nand\\nPurposes\\nof\\nTables:\\n1.\\nUserProfiles\\n:\\nStores\\nuser\\nprofile\\ninformation,\\nsuch\\nas\\nuser_id\\n,\\nname\\n,\\nemail\\n,\\ndate_of_birth\\n,\\nand\\njoin_date\\n.\\n2.\\nPosts\\n:\\nStores\\nposts\\ncreated\\nby\\nusers,\\nincluding\\nfields\\nfor\\npost_id\\n,\\nuser_id\\n(foreign\\nkey),\\ncontent\\n,\\npost_date\\n,\\nand\\nlikes_count\\n.\\n3.\\nComments\\n:\\nStores\\ncomments\\nmade\\non\\nposts,\\nwith\\nfields\\nlike\\ncomment_id\\n,\\npost_id\\n(foreign\\nkey),\\nuser_id\\n(foreign\\nkey),\\ncomment_text\\n,\\nand\\ncomment_date\\n.\\n4.\\nInteractions\\n:\\nTracks\\nuser\\ninteractions\\nwith\\nposts,\\nincluding\\ninteraction_id\\n,\\nuser_id\\n(foreign\\nkey),\\npost_id\\n(foreign\\nkey),\\ninteraction_type\\n(e.g.,\\nLike,\\nShare,\\nReact),\\nand\\ninteraction_date\\n.\\nData\\nPopulation:\\n●\\nThe\\ntables\\nwere\\npopulated\\nwith\\nvarious\\nrow\\ncounts:\\n10,000,\\n50,000,\\nand\\n100,000\\nfor\\nUserProfiles\\n,\\nPosts\\n,\\nComments\\n,\\nand\\n200,000\\nfor\\nInteractions\\n.\\n●\\nData\\nreflects\\nreal-world\\nscenarios\\nusing\\nrandomized\\ndates,\\nvaried\\ncontent,\\nand\\ninteraction\\ntypes.\\n●\\nRandomly\\nselected\\nrealistic\\nnames\\nand\\ncontent\\nsamples\\nwere\\nused\\nfor\\nvariety\\nin\\nthe\\nUserProfiles\\nand\\nPosts\\ntables.SQL\\nQueries\\nScripts\\nExecution\\nQuery\\n1:\\nRetrieve\\nposts\\ncreated\\nby\\na\\nspecific\\nuser.\\nSELECT\\npost_id,\\ncontent,\\npost_date,\\nlikes_count\\nFROM\\nPosts\\nWHERE\\nuser_id\\n=\\n100;\\nPurpose:\\nTo\\nfilter\\nand\\nretrieve\\nall\\nposts\\nmade\\nby\\na\\nparticular\\nuser.\\nQuery\\n2\\n:\\nRetrieve\\nthe\\ntop\\n10\\nmost-liked\\nposts.\\nSELECT\\npost_id,\\ncontent,\\nlikes_count\\nFROM\\nPosts\\nORDER\\nBY\\nlikes_count\\nDESC\\nOFFSET\\n0\\nROWS\\nFETCH\\nNEXT\\n10\\nROWS\\nONLY;\\nPurpose:\\nTo\\nidentify\\nthe\\nmost\\npopular\\nposts\\nbased\\non\\nthe\\nnumber\\nof\\nlikes\\nQuery\\n3:\\nRetrieve\\ncomments\\nmade\\nwithin\\na\\nspecific\\ndate\\nrange.\\nDECLARE\\n@startDate\\nDATE\\n=\\n\\'2022-01-02\\';\\nDECLARE\\n@endDate\\nDATE\\n=\\n\\'2022-12-31\\';\\nSELECT\\ncomment_id,\\npost_id,\\nuser_id,\\ncomment_text,\\ncomment_date\\nFROM\\nComments\\nWHERE\\ncomment_date\\nBETWEEN\\n@startDate\\nAND\\n@endDate;\\nPurpose:\\nTo\\nfilter\\ncomments\\nwithin\\na\\ndefined\\nperiod,\\nuseful\\nfor\\nanalyzing\\nactivity.Query\\n4\\n:\\nCount\\nthe\\nnumber\\nof\\nposts\\nmade\\nby\\neach\\nuser.\\nSELECT\\nuser_id,\\nCOUNT(post_id)\\nAS\\npost_count\\nFROM\\nPosts\\nGROUP\\nBY\\nuser_id\\nORDER\\nBY\\npost_count\\nDESC;\\nPurpose:\\nTo\\nidentify\\nactive\\nusers\\nbased\\non\\nthe\\nnumber\\nof\\nposts\\nthey\\nhave\\nmade.\\nQuery\\n5:\\nRetrieve\\ninteractions\\nfor\\na\\nspecific\\npost.\\nSELECT\\ninteraction_id,\\nuser_id,\\ninteraction_type,\\ninteraction_date\\nFROM\\nInteractions\\nWHERE\\npost_id\\n=\\n500;\\nPurpose:\\nTo\\nanalyze\\nthe\\ninteractions\\n(likes,\\nshares,\\netc.)\\non\\na\\nparticular\\npost.\\nGraphical\\nRepresentation\\nof\\nQuery\\nPerformance\\nThe\\nperformance\\nof\\neach\\nquery\\nwas\\nmeasured\\nby\\nexecuting\\nthem\\non\\ntables\\nwith\\ndifferent\\nsizes\\n(10,000,\\n50,000,\\nand\\n100,000\\nrows\\nfor\\nUserProfiles\\n,\\nPosts\\n,\\nand\\nComments\\n,\\nand\\n200,000\\nrows\\nfor\\nInteractions\\n).\\nThe\\nresponse\\ntimes\\nwere\\nrecorded.\\nimport\\npandas\\nas\\npd\\nimport\\nmatplotlib\\n.\\npyplot\\nas\\nplt\\n#\\nCreate\\na\\nDataFrame\\nto\\nrepresent\\nthe\\ncollected\\ndata\\ndata\\n=\\n{\\n\"Query\\nID\"\\n:\\n[\"Query\\n1\"\\n,\\n\"Query\\n1\"\\n,\\n\"Query\\n1\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n5\"\\n,\\n\"Query\\n5\"\\n,\\n\"Query\\n5\"\\n],\\n\"Data\\nSize\\n(Rows)\"\\n:\\n[\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n],\\n\"Response\\nTime\\n(ms)\"\\n:\\n[\\n120\\n,\\n450\\n,\\n900\\n,\\n130\\n,\\n480\\n,\\n950\\n,\\n110\\n,\\n500\\n,\\n870\\n,\\n140\\n,\\n510\\n,\\n960\\n,\\n150\\n,\\n530\\n,\\n980\\n]\\n}\\n#\\nConvert\\nthe\\ndictionary\\nto\\na\\nDataFrame\\ndf\\n=\\npd\\n.\\nDataFrame\\n(\\ndata\\n)#\\nPlotting\\nplt\\n.\\nfigure\\n(\\nfigsize\\n=\\n(\\n10\\n,\\n6\\n))\\n#\\nPlot\\neach\\nquery\\nfor\\nquery\\nin\\ndf\\n[\\n\\'Query\\nID\\'\\n].\\nunique\\n():\\nsubset\\n=\\ndf\\n[\\ndf\\n[\\n\\'Query\\nID\\'\\n]\\n==\\nquery\\n]\\nplt\\n.\\nplot\\n(\\nsubset\\n[\\n\\'Data\\nSize\\n(Rows)\\'\\n],\\nsubset\\n[\\n\\'Response\\nTime\\n(ms)\\'\\n],\\nmarker\\n=\\n\\'o\\'\\n,\\nlabel\\n=\\nquery\\n)\\n#\\nAdding\\ntitles\\nand\\nlabels\\nplt\\n.\\ntitle\\n(\\n\\'Response\\nTime\\nby\\nQuery\\nand\\nData\\nSize\\'\\n)\\nplt\\n.\\nxlabel\\n(\\n\\'Data\\nSize\\n(Rows)\\'\\n)\\nplt\\n.\\nylabel\\n(\\n\\'Response\\nTime\\n(ms)\\'\\n)\\nplt\\n.\\nlegend\\n()\\nplt\\n.\\ngrid\\n(\\nTrue\\n)\\nplt\\n.\\nxticks\\n(\\nsubset\\n[\\n\\'Data\\nSize\\n(Rows)\\'\\n])\\n#\\nShow\\nall\\ndata\\nsize\\npoints\\non\\nthe\\nx-axis\\n#\\nShow\\nthe\\nplot\\nplt\\n.\\nshow\\n()The\\ngraph\\nshows\\nthat\\nas\\nthe\\ndata\\nsize\\nincreases,\\nthe\\nresponse\\ntime\\nalso\\nincreases\\nfor\\nall\\nqueries,\\nwhich\\nis\\nexpected.\\nHowever,\\nsome\\nqueries,\\nsuch\\nas\\nthe\\none\\ninvolving\\naggregations\\n(\\nQuery\\n4\\n),\\nexhibit\\na\\nsteeper\\nincrease\\nin\\nresponse\\ntime\\ndue\\nto\\nthe\\ncomputational\\noverhead.\\nConclusion\\nThe\\nprovided\\nscripts\\nand\\nanalyses\\nsuccessfully\\nfulfill\\nthe\\ntasks\\'\\nrequirements:\\n1.\\nTables\\nwere\\ncreated\\nwith\\nrealistic\\ndata\\nthat\\nmimics\\nreal-world\\nscenarios.\\n2.\\nSQL\\nqueries\\nwere\\ndesigned\\nto\\nextract\\nmeaningful\\ninsights.\\n3.\\nQuery\\nperformance\\nwas\\nanalyzed\\nand\\ngraphically\\nrepresented,\\nshowing\\nhow\\ndata\\nsize\\nimpacts\\nexecution\\ntime.\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_vXGu5T_nDXU"
      },
      "outputs": [],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVvkAyM5nYFO",
        "outputId": "e68918c6-fb7f-43ae-8d9a-79535b12720f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-9c33d76eff6b>:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
            "<ipython-input-15-9c33d76eff6b>:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HreEpxkZn_nw"
      },
      "source": [
        "Create LangChain vector store...backed by Astra DB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1db7AmFMnraA"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s5kRux65oblQ"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# We need to split the text using Character Text Split such that it should be manageable\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(raw_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2cRwD0SouVU",
        "outputId": "376e97ac-e391-4015-d90d-9e87a1a4104f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Facebook\\nData\\nPerformance\\nAnalysis\\nReport\\nIntroduction\\nThis\\nreport\\naims\\nto\\nanalyze\\nthe\\nperformance\\nof\\nSQL\\nqueries\\non\\nvarious\\ndatasets\\nsimulating\\nFacebook-related\\ndata\\nsuch\\nas\\nuser\\nprofiles,\\nposts,\\ncomments,\\nand\\ninteractions.\\nThe\\ngoal\\nis\\nto\\nevaluate\\nthe\\nperformance\\nof\\neach\\nquery\\nacross\\ndifferent\\ndata\\nsizes,\\nusing\\nexecution\\ntimes\\nas\\na\\nmetric\\nfor\\ncomparison.\\nTable\\nDescriptions\\nand\\nData\\nPopulation\\n:\\nObjective\\n:\\nCreate\\nand\\npopulate\\ntables\\nto\\nstore\\nFacebook-related\\ndata,\\nreflecting\\nrealistic\\nscenarios.\\nTables\\nCreated\\nand\\nPurposes\\nof\\nTables:\\n1.\\nUserProfiles\\n:\\nStores\\nuser\\nprofile\\ninformation,\\nsuch\\nas\\nuser_id\\n,\\nname\\n,\\nemail\\n,\\ndate_of_birth\\n,\\nand\\njoin_date\\n.\\n2.\\nPosts\\n:\\nStores\\nposts\\ncreated\\nby\\nusers,\\nincluding\\nfields\\nfor\\npost_id\\n,\\nuser_id\\n(foreign\\nkey),\\ncontent\\n,\\npost_date\\n,\\nand\\nlikes_count\\n.\\n3.',\n",
              " 'as\\nuser_id\\n,\\nname\\n,\\nemail\\n,\\ndate_of_birth\\n,\\nand\\njoin_date\\n.\\n2.\\nPosts\\n:\\nStores\\nposts\\ncreated\\nby\\nusers,\\nincluding\\nfields\\nfor\\npost_id\\n,\\nuser_id\\n(foreign\\nkey),\\ncontent\\n,\\npost_date\\n,\\nand\\nlikes_count\\n.\\n3.\\nComments\\n:\\nStores\\ncomments\\nmade\\non\\nposts,\\nwith\\nfields\\nlike\\ncomment_id\\n,\\npost_id\\n(foreign\\nkey),\\nuser_id\\n(foreign\\nkey),\\ncomment_text\\n,\\nand\\ncomment_date\\n.\\n4.\\nInteractions\\n:\\nTracks\\nuser\\ninteractions\\nwith\\nposts,\\nincluding\\ninteraction_id\\n,\\nuser_id\\n(foreign\\nkey),\\npost_id\\n(foreign\\nkey),\\ninteraction_type\\n(e.g.,\\nLike,\\nShare,\\nReact),\\nand\\ninteraction_date\\n.\\nData\\nPopulation:\\n●\\nThe\\ntables\\nwere\\npopulated\\nwith\\nvarious\\nrow\\ncounts:\\n10,000,\\n50,000,\\nand\\n100,000\\nfor\\nUserProfiles\\n,\\nPosts\\n,\\nComments\\n,\\nand\\n200,000\\nfor\\nInteractions\\n.\\n●\\nData\\nreflects\\nreal-world\\nscenarios\\nusing\\nrandomized\\ndates,\\nvaried\\ncontent,\\nand',\n",
              " 'various\\nrow\\ncounts:\\n10,000,\\n50,000,\\nand\\n100,000\\nfor\\nUserProfiles\\n,\\nPosts\\n,\\nComments\\n,\\nand\\n200,000\\nfor\\nInteractions\\n.\\n●\\nData\\nreflects\\nreal-world\\nscenarios\\nusing\\nrandomized\\ndates,\\nvaried\\ncontent,\\nand\\ninteraction\\ntypes.\\n●\\nRandomly\\nselected\\nrealistic\\nnames\\nand\\ncontent\\nsamples\\nwere\\nused\\nfor\\nvariety\\nin\\nthe\\nUserProfiles\\nand\\nPosts\\ntables.SQL\\nQueries\\nScripts\\nExecution\\nQuery\\n1:\\nRetrieve\\nposts\\ncreated\\nby\\na\\nspecific\\nuser.\\nSELECT\\npost_id,\\ncontent,\\npost_date,\\nlikes_count\\nFROM\\nPosts\\nWHERE\\nuser_id\\n=\\n100;\\nPurpose:\\nTo\\nfilter\\nand\\nretrieve\\nall\\nposts\\nmade\\nby\\na\\nparticular\\nuser.\\nQuery\\n2\\n:\\nRetrieve\\nthe\\ntop\\n10\\nmost-liked\\nposts.\\nSELECT\\npost_id,\\ncontent,\\nlikes_count\\nFROM\\nPosts\\nORDER\\nBY\\nlikes_count\\nDESC\\nOFFSET\\n0\\nROWS\\nFETCH\\nNEXT\\n10\\nROWS\\nONLY;\\nPurpose:\\nTo\\nidentify\\nthe\\nmost\\npopular\\nposts\\nbased\\non\\nthe\\nnumber\\nof\\nlikes',\n",
              " \"posts.\\nSELECT\\npost_id,\\ncontent,\\nlikes_count\\nFROM\\nPosts\\nORDER\\nBY\\nlikes_count\\nDESC\\nOFFSET\\n0\\nROWS\\nFETCH\\nNEXT\\n10\\nROWS\\nONLY;\\nPurpose:\\nTo\\nidentify\\nthe\\nmost\\npopular\\nposts\\nbased\\non\\nthe\\nnumber\\nof\\nlikes\\nQuery\\n3:\\nRetrieve\\ncomments\\nmade\\nwithin\\na\\nspecific\\ndate\\nrange.\\nDECLARE\\n@startDate\\nDATE\\n=\\n'2022-01-02';\\nDECLARE\\n@endDate\\nDATE\\n=\\n'2022-12-31';\\nSELECT\\ncomment_id,\\npost_id,\\nuser_id,\\ncomment_text,\\ncomment_date\\nFROM\\nComments\\nWHERE\\ncomment_date\\nBETWEEN\\n@startDate\\nAND\\n@endDate;\\nPurpose:\\nTo\\nfilter\\ncomments\\nwithin\\na\\ndefined\\nperiod,\\nuseful\\nfor\\nanalyzing\\nactivity.Query\\n4\\n:\\nCount\\nthe\\nnumber\\nof\\nposts\\nmade\\nby\\neach\\nuser.\\nSELECT\\nuser_id,\\nCOUNT(post_id)\\nAS\\npost_count\\nFROM\\nPosts\\nGROUP\\nBY\\nuser_id\\nORDER\\nBY\\npost_count\\nDESC;\\nPurpose:\\nTo\\nidentify\\nactive\\nusers\\nbased\\non\\nthe\\nnumber\\nof\\nposts\\nthey\\nhave\\nmade.\\nQuery\\n5:\\nRetrieve\",\n",
              " 'SELECT\\nuser_id,\\nCOUNT(post_id)\\nAS\\npost_count\\nFROM\\nPosts\\nGROUP\\nBY\\nuser_id\\nORDER\\nBY\\npost_count\\nDESC;\\nPurpose:\\nTo\\nidentify\\nactive\\nusers\\nbased\\non\\nthe\\nnumber\\nof\\nposts\\nthey\\nhave\\nmade.\\nQuery\\n5:\\nRetrieve\\ninteractions\\nfor\\na\\nspecific\\npost.\\nSELECT\\ninteraction_id,\\nuser_id,\\ninteraction_type,\\ninteraction_date\\nFROM\\nInteractions\\nWHERE\\npost_id\\n=\\n500;\\nPurpose:\\nTo\\nanalyze\\nthe\\ninteractions\\n(likes,\\nshares,\\netc.)\\non\\na\\nparticular\\npost.\\nGraphical\\nRepresentation\\nof\\nQuery\\nPerformance\\nThe\\nperformance\\nof\\neach\\nquery\\nwas\\nmeasured\\nby\\nexecuting\\nthem\\non\\ntables\\nwith\\ndifferent\\nsizes\\n(10,000,\\n50,000,\\nand\\n100,000\\nrows\\nfor\\nUserProfiles\\n,\\nPosts\\n,\\nand\\nComments\\n,\\nand\\n200,000\\nrows\\nfor\\nInteractions\\n).\\nThe\\nresponse\\ntimes\\nwere\\nrecorded.\\nimport\\npandas\\nas\\npd\\nimport\\nmatplotlib\\n.\\npyplot\\nas\\nplt\\n#\\nCreate\\na\\nDataFrame\\nto\\nrepresent\\nthe',\n",
              " 'UserProfiles\\n,\\nPosts\\n,\\nand\\nComments\\n,\\nand\\n200,000\\nrows\\nfor\\nInteractions\\n).\\nThe\\nresponse\\ntimes\\nwere\\nrecorded.\\nimport\\npandas\\nas\\npd\\nimport\\nmatplotlib\\n.\\npyplot\\nas\\nplt\\n#\\nCreate\\na\\nDataFrame\\nto\\nrepresent\\nthe\\ncollected\\ndata\\ndata\\n=\\n{\\n\"Query\\nID\"\\n:\\n[\"Query\\n1\"\\n,\\n\"Query\\n1\"\\n,\\n\"Query\\n1\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n2\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n3\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n4\"\\n,\\n\"Query\\n5\"\\n,\\n\"Query\\n5\"\\n,\\n\"Query\\n5\"\\n],\\n\"Data\\nSize\\n(Rows)\"\\n:\\n[\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n,\\n10000\\n,\\n50000\\n,\\n100000\\n],\\n\"Response\\nTime\\n(ms)\"\\n:\\n[\\n120\\n,\\n450\\n,\\n900\\n,\\n130\\n,\\n480\\n,\\n950\\n,\\n110\\n,\\n500\\n,\\n870\\n,\\n140\\n,\\n510\\n,\\n960\\n,\\n150\\n,\\n530\\n,\\n980\\n]\\n}\\n#\\nConvert\\nthe\\ndictionary\\nto\\na\\nDataFrame\\ndf\\n=\\npd\\n.\\nDataFrame\\n(\\ndata\\n)#\\nPlotting\\nplt\\n.\\nfigure\\n(\\nfigsize\\n=\\n(\\n10\\n,\\n6',\n",
              " \",\\n450\\n,\\n900\\n,\\n130\\n,\\n480\\n,\\n950\\n,\\n110\\n,\\n500\\n,\\n870\\n,\\n140\\n,\\n510\\n,\\n960\\n,\\n150\\n,\\n530\\n,\\n980\\n]\\n}\\n#\\nConvert\\nthe\\ndictionary\\nto\\na\\nDataFrame\\ndf\\n=\\npd\\n.\\nDataFrame\\n(\\ndata\\n)#\\nPlotting\\nplt\\n.\\nfigure\\n(\\nfigsize\\n=\\n(\\n10\\n,\\n6\\n))\\n#\\nPlot\\neach\\nquery\\nfor\\nquery\\nin\\ndf\\n[\\n'Query\\nID'\\n].\\nunique\\n():\\nsubset\\n=\\ndf\\n[\\ndf\\n[\\n'Query\\nID'\\n]\\n==\\nquery\\n]\\nplt\\n.\\nplot\\n(\\nsubset\\n[\\n'Data\\nSize\\n(Rows)'\\n],\\nsubset\\n[\\n'Response\\nTime\\n(ms)'\\n],\\nmarker\\n=\\n'o'\\n,\\nlabel\\n=\\nquery\\n)\\n#\\nAdding\\ntitles\\nand\\nlabels\\nplt\\n.\\ntitle\\n(\\n'Response\\nTime\\nby\\nQuery\\nand\\nData\\nSize'\\n)\\nplt\\n.\\nxlabel\\n(\\n'Data\\nSize\\n(Rows)'\\n)\\nplt\\n.\\nylabel\\n(\\n'Response\\nTime\\n(ms)'\\n)\\nplt\\n.\\nlegend\\n()\\nplt\\n.\\ngrid\\n(\\nTrue\\n)\\nplt\\n.\\nxticks\\n(\\nsubset\\n[\\n'Data\\nSize\\n(Rows)'\\n])\\n#\\nShow\\nall\\ndata\\nsize\\npoints\\non\\nthe\\nx-axis\\n#\\nShow\\nthe\\nplot\\nplt\\n.\\nshow\\n()The\\ngraph\\nshows\\nthat\\nas\\nthe\\ndata\\nsize\\nincreases,\\nthe\\nresponse\\ntime\\nalso\",\n",
              " \"(\\nTrue\\n)\\nplt\\n.\\nxticks\\n(\\nsubset\\n[\\n'Data\\nSize\\n(Rows)'\\n])\\n#\\nShow\\nall\\ndata\\nsize\\npoints\\non\\nthe\\nx-axis\\n#\\nShow\\nthe\\nplot\\nplt\\n.\\nshow\\n()The\\ngraph\\nshows\\nthat\\nas\\nthe\\ndata\\nsize\\nincreases,\\nthe\\nresponse\\ntime\\nalso\\nincreases\\nfor\\nall\\nqueries,\\nwhich\\nis\\nexpected.\\nHowever,\\nsome\\nqueries,\\nsuch\\nas\\nthe\\none\\ninvolving\\naggregations\\n(\\nQuery\\n4\\n),\\nexhibit\\na\\nsteeper\\nincrease\\nin\\nresponse\\ntime\\ndue\\nto\\nthe\\ncomputational\\noverhead.\\nConclusion\\nThe\\nprovided\\nscripts\\nand\\nanalyses\\nsuccessfully\\nfulfill\\nthe\\ntasks'\\nrequirements:\\n1.\\nTables\\nwere\\ncreated\\nwith\\nrealistic\\ndata\\nthat\\nmimics\\nreal-world\\nscenarios.\\n2.\\nSQL\\nqueries\\nwere\\ndesigned\\nto\\nextract\\nmeaningful\\ninsights.\\n3.\\nQuery\\nperformance\\nwas\\nanalyzed\\nand\\ngraphically\\nrepresented,\\nshowing\\nhow\\ndata\\nsize\\nimpacts\\nexecution\\ntime.\"]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[:80]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT5RUHjqo-8v"
      },
      "source": [
        "Load the dataset into the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uFjhmhDo409",
        "outputId": "81854c07-5fcf-4a0b-928d-7be5b0c4561d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 8 headlines.\n"
          ]
        }
      ],
      "source": [
        "astra_vector_store.add_texts(texts[:80])\n",
        "\n",
        "print(\"Inserted %i headlines.\" % len(texts[:80]))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At5dC5VDpxPB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAAlM2PYpyzC"
      },
      "source": [
        "Run the QA cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnQhejM7p3ER",
        "outputId": "7514ab4c-2a26-45ef-8377-456b3df4bc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your question (or type 'quit' to exit): what is userId\n",
            "\n",
            "QUESTION: \"what is userId\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"userId is a foreign key used to identify and link users to their respective profiles, posts, comments, and interactions in the database. It is used as a unique identifier for each user.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "  [0.8493] \"various\n",
            "row\n",
            "counts:\n",
            "10,000,\n",
            "50,000,\n",
            "and\n",
            "100,000\n",
            "for\n",
            "UserProfiles\n",
            ",\n",
            "Posts\n",
            ",\n",
            "Comments\n",
            " ...\"\n",
            "  [0.8493] \"various\n",
            "row\n",
            "counts:\n",
            "10,000,\n",
            "50,000,\n",
            "and\n",
            "100,000\n",
            "for\n",
            "UserProfiles\n",
            ",\n",
            "Posts\n",
            ",\n",
            "Comments\n",
            " ...\"\n",
            "  [0.8464] \"as\n",
            "user_id\n",
            ",\n",
            "name\n",
            ",\n",
            "email\n",
            ",\n",
            "date_of_birth\n",
            ",\n",
            "and\n",
            "join_date\n",
            ".\n",
            "2.\n",
            "Posts\n",
            ":\n",
            "Stores\n",
            "posts\n",
            " ...\"\n",
            "  [0.8464] \"as\n",
            "user_id\n",
            ",\n",
            "name\n",
            ",\n",
            "email\n",
            ",\n",
            "date_of_birth\n",
            ",\n",
            "and\n",
            "join_date\n",
            ".\n",
            "2.\n",
            "Posts\n",
            ":\n",
            "Stores\n",
            "posts\n",
            " ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): what are the total  likes on posts\n",
            "\n",
            "QUESTION: \"what are the total  likes on posts\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"I don't have enough information to answer that question as the total likes on posts would depend on the specific posts being referenced in the query.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [0.8929] \"various\n",
            "row\n",
            "counts:\n",
            "10,000,\n",
            "50,000,\n",
            "and\n",
            "100,000\n",
            "for\n",
            "UserProfiles\n",
            ",\n",
            "Posts\n",
            ",\n",
            "Comments\n",
            " ...\"\n",
            "  [0.8929] \"various\n",
            "row\n",
            "counts:\n",
            "10,000,\n",
            "50,000,\n",
            "and\n",
            "100,000\n",
            "for\n",
            "UserProfiles\n",
            ",\n",
            "Posts\n",
            ",\n",
            "Comments\n",
            " ...\"\n",
            "  [0.8869] \"posts.\n",
            "SELECT\n",
            "post_id,\n",
            "content,\n",
            "likes_count\n",
            "FROM\n",
            "Posts\n",
            "ORDER\n",
            "BY\n",
            "likes_count\n",
            "DESC\n",
            "OFF ...\"\n",
            "  [0.8869] \"posts.\n",
            "SELECT\n",
            "post_id,\n",
            "content,\n",
            "likes_count\n",
            "FROM\n",
            "Posts\n",
            "ORDER\n",
            "BY\n",
            "likes_count\n",
            "DESC\n",
            "OFF ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): quit\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"  [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
